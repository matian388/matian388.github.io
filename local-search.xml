<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>什么是Spark</title>
    <link href="/2020/06/18/%E4%BB%80%E4%B9%88%E6%98%AFSpark/"/>
    <url>/2020/06/18/%E4%BB%80%E4%B9%88%E6%98%AFSpark/</url>
    
    <content type="html"><![CDATA[<h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>Spark是一个用来实现 <em>快速</em> 而 <em>通用</em> 的集群计算平台。</p><p>Spark提供的接口非常丰富。除了基于Python、Java、Scala和SQL的简单易用的API以及丰富的程序库外，Spark还能与其他大数据工具密切配合使用，例如Hadoop。</p><p>虽然Spark可以通过Python、Java或Scala使用，但它是用Scala写的，需要运行在Java环境当中。</p><h3 id="Docker中安装Spark"><a href="#Docker中安装Spark" class="headerlink" title="Docker中安装Spark"></a>Docker中安装Spark</h3>]]></content>
    
    
    <categories>
      
      <category>框架</category>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MapReduce</tag>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于Docker搭建Hadoop集群</title>
    <link href="/2020/06/17/%E5%9F%BA%E4%BA%8EDocker%E6%90%AD%E5%BB%BAHadoop%E9%9B%86%E7%BE%A4/"/>
    <url>/2020/06/17/%E5%9F%BA%E4%BA%8EDocker%E6%90%AD%E5%BB%BAHadoop%E9%9B%86%E7%BE%A4/</url>
    
    <content type="html"><![CDATA[<h2 id="0-绪论"><a href="#0-绪论" class="headerlink" title="0. 绪论"></a>0. 绪论</h2><p>使用Docker搭建Hadoop集群，需制作一个能运行Hadoop环境的镜像</p><p>本集群创建5个镜像分别命名为m1, s1, s2, s3, s4。其中m1为master，其他为slave。</p><h2 id="1-安装并使用Docker"><a href="#1-安装并使用Docker" class="headerlink" title="1. 安装并使用Docker"></a>1. 安装并使用Docker</h2><p>略（Todo 安装并使用Docker）</p><h2 id="2-制作镜像"><a href="#2-制作镜像" class="headerlink" title="2. 制作镜像"></a>2. 制作镜像</h2><p>从基础镜像（Ubuntu）新建一个容器</p><pre><code class="hljs applescript">sudo docker <span class="hljs-built_in">run</span> -<span class="hljs-keyword">it</span> ubuntu</code></pre><h3 id="2-1-安装Scala与Java"><a href="#2-1-安装Scala与Java" class="headerlink" title="2.1 安装Scala与Java"></a>2.1 安装Scala与Java</h3><pre><code class="hljs mipsasm">apt updateapt <span class="hljs-keyword">install </span>-y <span class="hljs-keyword">scala </span>openjdk<span class="hljs-number">-8</span>-<span class="hljs-keyword">jdk</span></code></pre><h3 id="2-2-安装ssh服务及客户端"><a href="#2-2-安装ssh服务及客户端" class="headerlink" title="2.2 安装ssh服务及客户端"></a>2.2 安装ssh服务及客户端</h3><pre><code class="hljs sql">apt <span class="hljs-keyword">install</span> -y openssh-<span class="hljs-keyword">server</span> openssh-<span class="hljs-keyword">client</span></code></pre><p>ssh安装完成后生成密钥对</p><pre><code class="hljs nginx"><span class="hljs-attribute">cd</span> ~ssh-keygen -t rsa</code></pre><p>一直回车直到密钥对创建完成，将公钥放入<code>~/.ssh/authorized_keys</code>中就可以容器间互相免密登录了</p><pre><code class="hljs jboss-cli">cat <span class="hljs-string">.ssh/id_rsa.pub</span> &gt;&gt; <span class="hljs-string">.ssh/authorized_keys</span></code></pre><p>修改 .bashrc 文件，启动 shell 的时候，自动启动 SSH 服务</p><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> <span class="hljs-string">'service ssh start'</span> &gt;&gt; ~/.bashrc</code></pre><h3 id="2-3-安装Hadoop"><a href="#2-3-安装Hadoop" class="headerlink" title="2.3 安装Hadoop"></a>2.3 安装Hadoop</h3><p>下载 Hadoop 的安装文件</p><pre><code class="hljs awk">wget http:<span class="hljs-regexp">//mi</span>rrors.hust.edu.cn<span class="hljs-regexp">/apache/</span>hadoop<span class="hljs-regexp">/common/</span>hadoop-<span class="hljs-number">3.2</span>.<span class="hljs-number">1</span><span class="hljs-regexp">/hadoop-3.2.1.tar.gz</span></code></pre><p>解压到 /usr/local 目录下面并重命名文件夹</p><pre><code class="hljs angelscript">tar -zxvf hadoop<span class="hljs-number">-3.2</span><span class="hljs-number">.1</span>.tar.gz -C /usr/local/cd /usr/local/mv hadoop<span class="hljs-number">-3.2</span><span class="hljs-number">.0</span> hadoop</code></pre><p>安装Vim并修改 /etc/profile 文件，添加一下环境变量到文件中</p><pre><code class="hljs vim">apt install -<span class="hljs-keyword">y</span> <span class="hljs-keyword">vim</span><span class="hljs-keyword">vim</span> /etc/<span class="hljs-keyword">profile</span></code></pre><p>追加以下内容</p><pre><code class="hljs routeros"><span class="hljs-comment">#java</span><span class="hljs-builtin-name">export</span> <span class="hljs-attribute">JAVA_HOME</span>=/usr/lib/jvm/java-8-openjdk-amd64<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">JRE_HOME</span>=<span class="hljs-variable">$&#123;JAVA_HOME&#125;</span>/jre    <span class="hljs-builtin-name">export</span> <span class="hljs-attribute">CLASSPATH</span>=.:$&#123;JAVA_HOME&#125;/lib:<span class="hljs-variable">$&#123;JRE_HOME&#125;</span>/lib    <span class="hljs-builtin-name">export</span> <span class="hljs-attribute">PATH</span>=<span class="hljs-variable">$&#123;JAVA_HOME&#125;</span>/bin:$PATH<span class="hljs-comment">#hadoop</span><span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HADOOP_HOME</span>=/usr/local/hadoop<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">PATH</span>=<span class="hljs-variable">$PATH</span>:$HADOOP_HOME/bin:$HADOOP_HOME/sbin<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HADOOP_COMMON_HOME</span>=<span class="hljs-variable">$HADOOP_HOME</span> <span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HADOOP_HDFS_HOME</span>=<span class="hljs-variable">$HADOOP_HOME</span> <span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HADOOP_MAPRED_HOME</span>=<span class="hljs-variable">$HADOOP_HOME</span><span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HADOOP_YARN_HOME</span>=<span class="hljs-variable">$HADOOP_HOME</span> <span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HADOOP_INSTALL</span>=<span class="hljs-variable">$HADOOP_HOME</span> <span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HADOOP_COMMON_LIB_NATIVE_DIR</span>=<span class="hljs-variable">$HADOOP_HOME</span>/lib/native <span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HADOOP_CONF_DIR</span>=<span class="hljs-variable">$HADOOP_HOME</span> <span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HADOOP_LIBEXEC_DIR</span>=<span class="hljs-variable">$HADOOP_HOME</span>/libexec <span class="hljs-builtin-name">export</span> <span class="hljs-attribute">JAVA_LIBRARY_PATH</span>=<span class="hljs-variable">$HADOOP_HOME</span>/lib/native:$JAVA_LIBRARY_PATH<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HADOOP_CONF_DIR</span>=<span class="hljs-variable">$HADOOP_PREFIX</span>/etc/hadoop<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HDFS_DATANODE_USER</span>=root<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HDFS_DATANODE_SECURE_USER</span>=root<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HDFS_SECONDARYNAMENODE_USER</span>=root<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HDFS_NAMENODE_USER</span>=root<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">YARN_RESOURCEMANAGER_USER</span>=root<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">YARN_NODEMANAGER_USER</span>=root</code></pre><p>使环境变量生效</p><pre><code class="hljs gradle"><span class="hljs-keyword">source</span> <span class="hljs-regexp">/etc/</span>profile</code></pre><p>追加/usr/local/hadoop/etc/hadoop/hadoop-env.sh文件，添加环境变量</p><pre><code class="hljs routeros"><span class="hljs-builtin-name">export</span> <span class="hljs-attribute">JAVA_HOME</span>=/usr/lib/jvm/java-8-openjdk-amd64<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HDFS_NAMENODE_USER</span>=root<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HDFS_DATANODE_USER</span>=root<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HDFS_SECONDARYNAMENODE_USER</span>=root<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">YARN_RESOURCEMANAGER_USER</span>=root<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">YARN_NODEMANAGER_USER</span>=root</code></pre><p>修改 /usr/local/hadoop/etc/hadoop/core-site.xml，添加配置</p><pre><code class="hljs dts"><span class="hljs-params">&lt;configuration&gt;</span>    <span class="hljs-params">&lt;property&gt;</span>        <span class="hljs-params">&lt;name&gt;</span>fs.default.name<span class="hljs-params">&lt;/name&gt;</span>        <span class="hljs-params">&lt;value&gt;</span>hdfs:<span class="hljs-comment">//m1:9000&lt;/value&gt;</span>    <span class="hljs-params">&lt;/property&gt;</span>    <span class="hljs-params">&lt;property&gt;</span>        <span class="hljs-params">&lt;name&gt;</span>hadoop.tmp.dir<span class="hljs-params">&lt;/name&gt;</span>        <span class="hljs-params">&lt;value&gt;</span><span class="hljs-meta-keyword">/home/</span>hadoop3<span class="hljs-meta-keyword">/hadoop/</span>tmp<span class="hljs-params">&lt;/value&gt;</span>    <span class="hljs-params">&lt;/property&gt;</span><span class="hljs-params">&lt;/configuration&gt;</span></code></pre><p>修改 /usr/local/hadoop/etc/hadoop/hdfs-site.xml<br>dfs.replication 表示备份的数量，本例设置为2份</p><pre><code class="hljs dts"><span class="hljs-params">&lt;configuration&gt;</span>    <span class="hljs-params">&lt;property&gt;</span>        <span class="hljs-params">&lt;name&gt;</span>dfs.replication<span class="hljs-params">&lt;/name&gt;</span>        <span class="hljs-params">&lt;value&gt;</span><span class="hljs-number">2</span><span class="hljs-params">&lt;/value&gt;</span>    <span class="hljs-params">&lt;/property&gt;</span>    <span class="hljs-params">&lt;property&gt;</span>        <span class="hljs-params">&lt;name&gt;</span>dfs.namenode.name.dir<span class="hljs-params">&lt;/name&gt;</span>        <span class="hljs-params">&lt;value&gt;</span><span class="hljs-meta-keyword">/home/</span>hadoop3<span class="hljs-meta-keyword">/hadoop/</span>hdfs/name<span class="hljs-params">&lt;/value&gt;</span>    <span class="hljs-params">&lt;/property&gt;</span>    <span class="hljs-params">&lt;property&gt;</span>        <span class="hljs-params">&lt;name&gt;</span>dfs.namenode.data.dir<span class="hljs-params">&lt;/name&gt;</span>        <span class="hljs-params">&lt;value&gt;</span><span class="hljs-meta-keyword">/home/</span>hadoop3<span class="hljs-meta-keyword">/hadoop/</span>hdfs/data<span class="hljs-params">&lt;/value&gt;</span>    <span class="hljs-params">&lt;/property&gt;</span><span class="hljs-params">&lt;/configuration&gt;</span></code></pre><p>修改 /usr/local/hadoop/etc/hadoop/mapred-site.xml</p><pre><code class="hljs crystal">&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;mapreduce.framework.name&lt;<span class="hljs-regexp">/name&gt;</span><span class="hljs-regexp">        &lt;value&gt;yarn&lt;/value</span>&gt;    &lt;<span class="hljs-regexp">/property&gt;</span><span class="hljs-regexp">    &lt;property&gt;</span><span class="hljs-regexp">        &lt;name&gt;mapreduce.application.classpath&lt;/name</span>&gt;        &lt;value&gt;            /usr/local/hadoop/etc/hadoop,            /usr/local/hadoop/share/hadoop/common/*,            /usr/local/hadoop/share/hadoop/common/<span class="hljs-class"><span class="hljs-keyword">lib</span>/*,</span>            /usr/local/hadoop/share/hadoop/hdfs/*,            /usr/local/hadoop/share/hadoop/hdfs/<span class="hljs-class"><span class="hljs-keyword">lib</span>/*,</span>            /usr/local/hadoop/share/hadoop/mapreduce/*,            /usr/local/hadoop/share/hadoop/mapreduce/<span class="hljs-class"><span class="hljs-keyword">lib</span>/*,</span>            /usr/local/hadoop/share/hadoop/yarn/*,            /usr/local/hadoop/share/hadoop/yarn/<span class="hljs-class"><span class="hljs-keyword">lib</span>/*</span>        &lt;<span class="hljs-regexp">/value&gt;</span><span class="hljs-regexp">    &lt;/property</span>&gt;&lt;<span class="hljs-regexp">/configuration&gt;</span></code></pre><p>修改 /usr/local/hadoop/etc/hadoop/yarn-site.xml，配置master name</p><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>m1<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>mapreduce_shuffle<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span></code></pre><p>最后修改 /usr/local/hadoop/etc/hadoop/workers 指定slave name</p><pre><code class="hljs mipsasm"><span class="hljs-built_in">s1</span><span class="hljs-built_in">s2</span><span class="hljs-built_in">s3</span><span class="hljs-built_in">s4</span></code></pre><p>此时，hadoop已经配置好了</p><h3 id="2-4-提交镜像"><a href="#2-4-提交镜像" class="headerlink" title="2.4 提交镜像"></a>2.4 提交镜像</h3><pre><code class="hljs armasm"><span class="hljs-symbol">sudo</span> docker commit -m <span class="hljs-string">"haddop"</span> <span class="hljs-keyword">b95a8ccf5568 </span>hadoop</code></pre><h2 id="3-启动Hadoop集群"><a href="#3-启动Hadoop集群" class="headerlink" title="3. 启动Hadoop集群"></a>3. 启动Hadoop集群</h2><h3 id="3-1-创建Hadoop集群容器"><a href="#3-1-创建Hadoop集群容器" class="headerlink" title="3.1 创建Hadoop集群容器"></a>3.1 创建Hadoop集群容器</h3><p>首先我们需要创建一个 Hadoop 集群的虚拟网络，现在的 Docker 网络能够提供 DNS 解析功能，我们可以使用如下命令构建。</p><pre><code class="hljs routeros">sudo docker<span class="hljs-built_in"> network </span>create <span class="hljs-attribute">--driver</span>=bridge hadoop</code></pre><p>然后启动<code>m1</code>作为Hadoop的master容器</p><pre><code class="hljs angelscript">sudo docker run -itd --network hadoop -h m1 --name <span class="hljs-string">"m1"</span> -p <span class="hljs-number">9870</span>:<span class="hljs-number">9870</span> -p <span class="hljs-number">8088</span>:<span class="hljs-number">8088</span> hadoop</code></pre><p>接下来启动4个slave容器: s1, s2, s3, s4</p><pre><code class="hljs armasm"><span class="hljs-symbol">sudo</span> docker run -<span class="hljs-keyword">itd </span>--network hadoop -h <span class="hljs-built_in">s1</span> --name <span class="hljs-string">"s1"</span> hadoop<span class="hljs-symbol">sudo</span> docker run -<span class="hljs-keyword">itd </span>--network hadoop -h <span class="hljs-built_in">s2</span> --name <span class="hljs-string">"s2"</span> hadoop<span class="hljs-symbol">sudo</span> docker run -<span class="hljs-keyword">itd </span>--network hadoop -h <span class="hljs-built_in">s3</span> --name <span class="hljs-string">"s3"</span> hadoop<span class="hljs-symbol">sudo</span> docker run -<span class="hljs-keyword">itd </span>--network hadoop -h <span class="hljs-built_in">s4</span> --name <span class="hljs-string">"s4"</span> hadoop</code></pre><h3 id="3-2-启动Hadoop集群"><a href="#3-2-启动Hadoop集群" class="headerlink" title="3.2 启动Hadoop集群"></a>3.2 启动Hadoop集群</h3><p>进入master中格式化HDFS</p><pre><code class="hljs jboss-cli">sudo docker attach m1<span class="hljs-keyword">cd</span> <span class="hljs-string">/usr/local/hadoop/bin</span><span class="hljs-string">./hadoop</span> namenode -format</code></pre><p>启动Hadoop进程</p><pre><code class="hljs jboss-cli"><span class="hljs-keyword">cd</span> <span class="hljs-string">/usr/local/hadoop/sbin/</span><span class="hljs-string">./start-all.sh</span></code></pre><p>即能看到输出表明集群启动成功，访问本机的 8088 与 9870 端口就可以看到监控信息了</p><pre><code class="hljs groovy">Starting namenodes on [m1]<span class="hljs-string">m1:</span> <span class="hljs-string">Warning:</span> Permanently added <span class="hljs-string">'m1,172.18.0.2'</span> (ECDSA) to the list of known hosts.Starting datanodes<span class="hljs-string">s1:</span> <span class="hljs-string">Warning:</span> Permanently added <span class="hljs-string">'s1,172.18.0.3'</span> (ECDSA) to the list of known hosts.<span class="hljs-string">s3:</span> <span class="hljs-string">Warning:</span> Permanently added <span class="hljs-string">'s3,172.18.0.5'</span> (ECDSA) to the list of known hosts.<span class="hljs-string">s2:</span> <span class="hljs-string">Warning:</span> Permanently added <span class="hljs-string">'s2,172.18.0.4'</span> (ECDSA) to the list of known hosts.<span class="hljs-string">s4:</span> <span class="hljs-string">Warning:</span> Permanently added <span class="hljs-string">'s4,172.18.0.6'</span> (ECDSA) to the list of known hosts.<span class="hljs-string">s4:</span> <span class="hljs-string">WARNING:</span> <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/hadoop/</span>logs does not exist. Creating.<span class="hljs-string">s2:</span> <span class="hljs-string">WARNING:</span> <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/hadoop/</span>logs does not exist. Creating.<span class="hljs-string">s3:</span> <span class="hljs-string">WARNING:</span> <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/hadoop/</span>logs does not exist. Creating.<span class="hljs-string">s1:</span> <span class="hljs-string">WARNING:</span> <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/hadoop/</span>logs does not exist. Creating.Starting secondary namenodes [m1]Starting resourcemanagerStarting nodemanagers</code></pre><p><img src="https://i.loli.net/2020/06/17/3MtX5NcE96FngaQ.jpg" srcset="/img/loading.gif" alt="localhost:8088"></p>]]></content>
    
    
    <categories>
      
      <category>框架</category>
      
      <category>Hadoop</category>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hadoop</tag>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MapReduce简介</title>
    <link href="/2020/06/17/MapReduce%E7%AE%80%E4%BB%8B/"/>
    <url>/2020/06/17/MapReduce%E7%AE%80%E4%BB%8B/</url>
    
    <content type="html"><![CDATA[<h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>MapReduce是Google提出的一个软件架构，用于大数据的并行运算。其中”Map(映射)”，”Reduce(归纳)”和主要思想均从函数式编程借鉴来的。</p><p>每个文件分片由单独的机器去处理，这就是Map的方法；将各个机器计算的结果汇总并得到最终的结果，这就是Reduce的方法。</p><h3 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h3><p>向MapReduce框架提交一个计算作业时，它会首先把计算作业拆分成若干个Map任务，然后分配到不同的节点上去执行，每一个Map任务处理输入数据中的一部分，当Map任务完成后，它会生成一些中间文件，这些中间文件将会作为Reduce任务的输入数据。Reduce任务的主要目标就是把前面若干个Map的输出汇总到一起并输出。</p><p><img src="https://cs.calvin.edu/courses/cs/374/exercises/12/lab/MapReduceWordCount.png" srcset="/img/loading.gif" alt="MapReduce工作流程"></p>]]></content>
    
    
    <categories>
      
      <category>框架</category>
      
      <category>Hadoop</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hadoop</tag>
      
      <tag>MapReduce</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HDFS框架</title>
    <link href="/2020/06/16/HDFS%E6%A1%86%E6%9E%B6/"/>
    <url>/2020/06/16/HDFS%E6%A1%86%E6%9E%B6/</url>
    
    <content type="html"><![CDATA[<h3 id="HDFS框架分析"><a href="#HDFS框架分析" class="headerlink" title="HDFS框架分析"></a>HDFS框架分析</h3><p>HDFS和大多数分布式框架结构一致，都是由master（Name-Node、Secondary NameNode）/slave（DataNode）构成。</p><p><img src="https://pic2.zhimg.com/80/v2-5ec14190630b72a07a242787a1e874cd_1440w.jpg" srcset="/img/loading.gif" alt="HDFS结构"></p><h3 id="master的主要功能"><a href="#master的主要功能" class="headerlink" title="master的主要功能"></a>master的主要功能</h3><ul><li>管理HDFS文件系统的namespace，负责client请求（读取、存储）的相应</li><li>管理文件存储到slave节点上（包含数据副本的分配）</li><li>按配置（blocksize）项拆分文件成多个block（默认128M）存储到多个slave节点上</li></ul><h3 id="slave的主要功能"><a href="#slave的主要功能" class="headerlink" title="slave的主要功能"></a>slave的主要功能</h3><ul><li>负责block（数据块）的创建、删除以及副本存储工作</li><li>定期向master发送心跳信息汇报节点状态</li></ul><h3 id="HDFS的master容错机制"><a href="#HDFS的master容错机制" class="headerlink" title="HDFS的master容错机制"></a>HDFS的master容错机制</h3><p>Hadoop可以配置高可用集群，集群中有两个master节点，一台Name-Node主节点，另一台Secondary NameNode备用节点，两者数据时刻保持一致。当主节点不可用时，备用节点马上自动切换，用户感知不到，避免了master的单点问题。</p><h3 id="HDFS的数据块副本机制"><a href="#HDFS的数据块副本机制" class="headerlink" title="HDFS的数据块副本机制"></a>HDFS的数据块副本机制</h3><p>超过一定大小（blocksize）的文件会被拆分成多个block存储在不同的slave里。HDFS为了容错会把一个block存储在多个slave当中，而一个block存到几个slave当中是由副本因子（replication factor）配置的。</p><p><img src="https://pic3.zhimg.com/80/v2-00887ec98b314356fd05c0b60df6057a_1440w.jpg" srcset="/img/loading.gif" alt="HDFS副本机制"></p><h3 id="HDFS读文件流程"><a href="#HDFS读文件流程" class="headerlink" title="HDFS读文件流程"></a>HDFS读文件流程</h3><ul><li>由client向master查询元数据，找到文件的block所在的slave</li><li>挑选一台slave服务器建立连接</li><li>client按packet为单位接收并缓存，最终合成所需文件</li></ul><h3 id="HDFS写文件流程"><a href="#HDFS写文件流程" class="headerlink" title="HDFS写文件流程"></a>HDFS写文件流程</h3><ul><li>由client向master发送上传文件请求</li><li>client按master要求拆分文件</li><li>client向master所给的slave上传第一个block</li><li>slave接收完block校验并发送至其他slave做副本备份</li><li>client再次向master请求下一个block上传的slave</li></ul>]]></content>
    
    
    <categories>
      
      <category>框架</category>
      
      <category>Hadoop</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hadoop</tag>
      
      <tag>HDFS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>初识Hadoop</title>
    <link href="/2020/06/16/%E5%88%9D%E8%AF%86Hadoop/"/>
    <url>/2020/06/16/%E5%88%9D%E8%AF%86Hadoop/</url>
    
    <content type="html"><![CDATA[<h2 id="Hadoop是啥"><a href="#Hadoop是啥" class="headerlink" title="Hadoop是啥"></a>Hadoop是啥</h2><p>Hadoop是一个开源大数据框架，是一个分布式计算的解决方案。</p><p>Hadoop解决了大数据存储（HDFS文件系统）和分布式计算（MapReduce）两大难题。</p><p><img src="http://www.sunlab.org/teaching/cse8803/fall2016/lab/image/post/mapreduce-flow.jpg" srcset="/img/loading.gif" alt="Hadoop结构"></p><h2 id="Hadoop特点"><a href="#Hadoop特点" class="headerlink" title="Hadoop特点"></a>Hadoop特点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul><li>HDFS<ul><li>支持超大文件至PB级别</li><li>分布式可快速扩容呈线性扩展</li><li>有冗余备份</li><li>具有检测和快速应对故障的能力</li></ul></li><li>MapReduce<ul><li>开发简单</li><li>可扩展性强</li><li>对节点故障容错性强</li></ul></li></ul><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul><li>HDFS<ul><li>不适合大量小文件存储</li><li>文件修改效率低</li><li>速度慢</li></ul></li><li>MapReduce<ul><li>执行时间长</li><li>Map和Reduce函数过于底层</li><li>不能实现一些算法</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>框架</category>
      
      <category>Hadoop</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hadoop</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
