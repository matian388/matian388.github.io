<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>利用PySpark进行RDD编程 -- spark学习笔记（一）</title>
    <link href="/2020/06/20/%E5%88%A9%E7%94%A8PySpark%E8%BF%9B%E8%A1%8CRDD%E7%BC%96%E7%A8%8B%20--%20spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <url>/2020/06/20/%E5%88%A9%E7%94%A8PySpark%E8%BF%9B%E8%A1%8CRDD%E7%BC%96%E7%A8%8B%20--%20spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><p>可以根据我的这篇文章: <a href="/2020/06/18/%E5%9C%A8Docker%E4%B8%AD%E5%AE%89%E8%A3%85PySpark/" title="在Docker中安装PySpark">在Docker中安装PySpark</a>，创建一个PySpark容器在jupyter lab中进行尝试</p><p>或者</p><p>在这个<a href="https://community.cloud.databricks.com/" target="_blank" rel="noopener">免费的spark平台</a>注册一个账户，创建集群进行编程，有16GB的内存对于初学是够用的了，具体创建方法可参考<a href="https://zhuanlan.zhihu.com/p/143151136" target="_blank" rel="noopener">这篇文章</a></p><h3 id="创建RDD"><a href="#创建RDD" class="headerlink" title="创建RDD"></a>创建RDD</h3><p>Spark提供了两种创建RDD的方式：读取外部数据集和在驱动器程序中对一个集合进行并行化创建</p><h4 id="方法一：读取外部数据集"><a href="#方法一：读取外部数据集" class="headerlink" title="方法一：读取外部数据集"></a>方法一：读取外部数据集</h4><p><code>sc.textFile(&quot;filename&quot;)</code></p><p>这个方法是以后比较常用的方法，毕竟大文件都需要从外部读取而不能用内存加载进来。</p><p>我们先随便下载一个Spark的<a href="https://raw.githubusercontent.com/apache/spark/master/README.md" target="_blank" rel="noopener">README文件</a>并放入服务器中：/FileStore/tables/README.md</p><pre><code class="hljs ini"><span class="hljs-attr">lines</span> = sc.textFile(<span class="hljs-string">"/FileStore/tables/README.md"</span>)</code></pre><p>基于Spark的惰性计算，RDD并不会把文件读入内存，而是在真正需要输出时再读取从而可以对内存与cpu的使用更加合理。</p><p>比如接下来运行<code>lines.first()</code>读取第一行时，Spark就只会读取README.me的一行从而返回结果，这个pandas是很不一样的。</p><h4 id="方法二：在驱动器程序中对一个集合进行并行化创建"><a href="#方法二：在驱动器程序中对一个集合进行并行化创建" class="headerlink" title="方法二：在驱动器程序中对一个集合进行并行化创建"></a>方法二：在驱动器程序中对一个集合进行并行化创建</h4><p><code>sc.parallelize([])</code></p><p>这是创建一个RDD最简单的方式，在学习Spark时会经常使用</p><pre><code class="hljs ini"><span class="hljs-attr">lines</span> = sc.parallelize([<span class="hljs-string">'spark'</span>, <span class="hljs-string">'i like spark'</span>])</code></pre>]]></content>
    
    
    <categories>
      
      <category>框架</category>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
      <tag>PySpark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>在Docker中安装PySpark</title>
    <link href="/2020/06/18/%E5%9C%A8Docker%E4%B8%AD%E5%AE%89%E8%A3%85PySpark/"/>
    <url>/2020/06/18/%E5%9C%A8Docker%E4%B8%AD%E5%AE%89%E8%A3%85PySpark/</url>
    
    <content type="html"><![CDATA[<h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>Apache Spark 是一种用于处理、查询和分析大数据的快速集群计算框架。Apache Spark 是基于内存计算，这是他与其他几种大数据框架相比的一大优势。Apache Spark 是开源的，也是最著名的大数据框架之一。当它使用内存计算时，它比传统map-reduce任务快100倍；当它使用磁盘时比传统的map-reduce 任务快10倍。</p><p>Spark提供的接口非常丰富。除了基于Python、Java、Scala和SQL的简单易用的API以及丰富的程序库外，Spark还能与其他大数据工具密切配合使用，例如Hadoop。</p><p>虽然Spark可以通过Python、Java或Scala使用，但它是用Scala写的，需要运行在Java环境当中。</p><p>PySpark是Spark为Python开发者提供的API。</p><h2 id="安装Spark"><a href="#安装Spark" class="headerlink" title="安装Spark"></a>安装Spark</h2><p>从基础镜像新建一个容器</p><pre><code class="hljs applescript">sudo docker <span class="hljs-built_in">run</span> -<span class="hljs-keyword">it</span> <span class="hljs-comment">--name spark ubuntu</span>apt update</code></pre><p>安装python3、Java环境和scala并测试</p><pre><code class="hljs mipsasm">apt <span class="hljs-keyword">install </span>-y python3 python3-pipapt <span class="hljs-keyword">install </span>-y openjdk<span class="hljs-number">-8</span>-<span class="hljs-keyword">jdk</span><span class="hljs-keyword">java </span>-versionapt <span class="hljs-keyword">install </span>-y <span class="hljs-keyword">scala</span><span class="hljs-keyword">scala </span>-version</code></pre><p>接下来去<a href="http://spark.apache.org/downloads.html" target="_blank" rel="noopener">官网</a>下载最新版的Spark并安装</p><pre><code class="hljs angelscript">apt install -y wgetwget https:<span class="hljs-comment">//mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-3.0.0-preview2/spark-3.0.0-preview2-bin-hadoop2.7.tgz</span>tar xvf spark<span class="hljs-number">-3.0</span><span class="hljs-number">.0</span>-preview2-bin-hadoop2<span class="hljs-number">.7</span>.tgzrm spark<span class="hljs-number">-3.0</span><span class="hljs-number">.0</span>-preview2-bin-hadoop2<span class="hljs-number">.7</span>.tgzmv spark<span class="hljs-number">-3.0</span><span class="hljs-number">.0</span>-preview2-bin-hadoop2<span class="hljs-number">.7</span>.tgz /usr/local/spark/</code></pre><p>最后把Spark加入环境变量</p><pre><code class="hljs properties"><span class="hljs-attr">apt</span> <span class="hljs-string">install -y vim</span><span class="hljs-attr">vim</span> <span class="hljs-string">~/.bashrc</span></code></pre><p>添加SPARK_HOME与PATH（具体SPARK_HOME路径自己设定）</p><pre><code class="hljs routeros"><span class="hljs-builtin-name">export</span> <span class="hljs-attribute">SPARK_HOME</span>=/usr/local/spark<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">PATH</span>=<span class="hljs-variable">$&#123;SPARK_HOME&#125;</span>/bin:$PATH</code></pre><p>激活环境变量</p><pre><code class="hljs bash"><span class="hljs-built_in">source</span> ~/.bashrc</code></pre><p>然后会发现pyspark跑不起来</p><h2 id="填坑并启动PySpark"><a href="#填坑并启动PySpark" class="headerlink" title="填坑并启动PySpark"></a>填坑并启动PySpark</h2><p>首先Py4J在驱动程序上用于Python和Java SparkContext对象之间的本地通信，大型数据传输是通过不同的机制执行的。<br>所以需要用pip3安装py4j</p><pre><code class="hljs cmake">apt <span class="hljs-keyword">install</span> -y python3-pippip3 <span class="hljs-keyword">install</span> py4j</code></pre><p>然后会发现pyspark依然跑不起来<br>是因为我用的是python3<br>所以需要添加python相关环境变量</p><pre><code class="hljs jboss-cli">vim ~<span class="hljs-string">/.bashrc</span></code></pre><p>添加如下变量</p><pre><code class="hljs crystal">export PYTHONPATH=$SPARK_HOME/<span class="hljs-symbol">python:</span>$SPARK_HOME/python/<span class="hljs-class"><span class="hljs-keyword">lib</span>/<span class="hljs-title">py4j</span>-0.10.8.1-<span class="hljs-title">src</span>.<span class="hljs-title">zip</span>:$<span class="hljs-title">PYTHONPATH</span></span>export PYSPARK_PYTHON=python3</code></pre><p>注意：py4j-0.10.8.1-src.zip要到$SPARK_HOME/python/lib目录查看是否是这个名称。不同版本的py4j的名称会有差别</p><p>最后激活环境变量</p><pre><code class="hljs armasm"><span class="hljs-symbol">source</span> ~/.<span class="hljs-keyword">bashrc</span><span class="hljs-keyword">pyspark</span></code></pre><p><img src="https://i.loli.net/2020/06/18/ApoqKVFbMjP8JEv.png" srcset="/img/loading.gif" alt="pyspark"><br>终于跑起来了，然后可以跑一下example</p><pre><code class="hljs applescript">/usr/<span class="hljs-keyword">local</span>/spark/bin/<span class="hljs-built_in">run</span>-example SparkPi <span class="hljs-number">10</span></code></pre><p>会发现输出了好多INFO级别的log，如果不想看那么多日志，就需要配置log4j的日志打印详细级别</p><pre><code class="hljs routeros">cd /usr/local/spark/cp conf/log4j.properties.template conf/log4j.propertiesvim conf/log4j.properties将 log4j.<span class="hljs-attribute">rootCategory</span>=INFO,<span class="hljs-built_in"> console </span> 替换为log4j.<span class="hljs-attribute">rootCategory</span>=ERROR, console</code></pre><p>这个WARNING是因为Java KDJ版本高于8造成的，可以忽略</p><pre><code class="hljs asciidoc"><span class="hljs-symbol">WARNING: </span>An illegal reflective access operation has occurred<span class="hljs-symbol">WARNING: </span>Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark/jars/spark-unsafe<span class="hljs-emphasis">_2.12-3.0.0-preview2.jar) to constructor java.nio.DirectByteBuffer(long,int)</span><span class="hljs-emphasis">WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform</span><span class="hljs-emphasis">WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations</span><span class="hljs-emphasis">WARNING: All illegal access operations will be denied in a future release</span></code></pre><h2 id="在jupyter-notebook中使用pyspark"><a href="#在jupyter-notebook中使用pyspark" class="headerlink" title="在jupyter notebook中使用pyspark"></a>在jupyter notebook中使用pyspark</h2><p>首先安装jupyter lab</p><pre><code class="hljs cmake">pip3 <span class="hljs-keyword">install</span> jupyterlab</code></pre><p>然后利用jupyter的命令创建配置文件</p><pre><code class="hljs verilog">jupyter notebook --<span class="hljs-keyword">generate</span>-<span class="hljs-keyword">config</span></code></pre><p>进入配置文件配置</p><pre><code class="hljs ini"><span class="hljs-attr">c.NotebookApp.ip</span> = <span class="hljs-string">'0.0.0.0'</span> <span class="hljs-comment"># 让外部可以访问</span><span class="hljs-attr">c.NotebookApp.port</span> = <span class="hljs-number">8888</span> <span class="hljs-comment"># 监听端口</span></code></pre><p>然后利用jupyter的命令初始命令这样以后就不用每次新建容器的时候重设密码了</p><pre><code class="hljs ebnf"><span class="hljs-attribute">jupyter notebook passwd</span></code></pre><p>在Jupyter Notebook中使用PySpark有一种通用的方法：使用findSpark包在代码中提供Spark Context。<br>findSpark包不是特定于Jupyter Notebook，你也可以在你喜欢的IDE中使用这个技巧。 要安装findspark</p><pre><code class="hljs cmake">pip3 <span class="hljs-keyword">install</span> findspark</code></pre><p>之后就可以ctrl+d退出容器创建pyspark镜像啦</p><pre><code class="hljs ebnf"><span class="hljs-attribute">sudo docker commit spark pyspark</span></code></pre><p>接下来就是最后一步启动pyspark容器</p><pre><code class="hljs dockerfile">sudo docker <span class="hljs-keyword">run</span><span class="bash"> -itd --name pyspark -p 8889:8888 pyspark bash -c <span class="hljs-string">'cd ~&amp;&amp;jupyter lab --allow-root'</span></span></code></pre><p>-p 端口映射前面8889是宿主机端口，后面的8888是容器，因为宿主机的8888已被占用我就随便写了8889<br>由于容器默认是root用户，而jupyter lab默认是不允许root用户启动的，所以需添加<code>--allow-root</code></p><p>这样我们就可以从宿主机的ip:8889访问到能运行pyspark的jupyter lab了</p><p>让我们测试一下</p><pre><code class="hljs haskell"><span class="hljs-title">from</span> pyspark <span class="hljs-keyword">import</span> SparkContext<span class="hljs-title">from</span> pyspark <span class="hljs-keyword">import</span> SparkConf<span class="hljs-title">sc</span> = <span class="hljs-type">SparkContext</span>()<span class="hljs-class"><span class="hljs-keyword">data</span> = list(<span class="hljs-title">range</span>(1,100))</span><span class="hljs-title">rdd</span> = sc.parallelize(<span class="hljs-class"><span class="hljs-keyword">data</span>)</span><span class="hljs-title">rdd</span>.collect()</code></pre><p><img src="https://i.loli.net/2020/06/20/Up9hP3iGkMvmrWe.png" srcset="/img/loading.gif" alt="PySpark"></p><p>完美！</p>]]></content>
    
    
    <categories>
      
      <category>框架</category>
      
      <category>Spark</category>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MapReduce</tag>
      
      <tag>Spark</tag>
      
      <tag>Docker</tag>
      
      <tag>PySpark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于Docker搭建Hadoop集群</title>
    <link href="/2020/06/17/%E5%9F%BA%E4%BA%8EDocker%E6%90%AD%E5%BB%BAHadoop%E9%9B%86%E7%BE%A4/"/>
    <url>/2020/06/17/%E5%9F%BA%E4%BA%8EDocker%E6%90%AD%E5%BB%BAHadoop%E9%9B%86%E7%BE%A4/</url>
    
    <content type="html"><![CDATA[<h2 id="0-绪论"><a href="#0-绪论" class="headerlink" title="0. 绪论"></a>0. 绪论</h2><p>使用Docker搭建Hadoop集群，需制作一个能运行Hadoop环境的镜像</p><p>本集群创建5个镜像分别命名为m1, s1, s2, s3, s4。其中m1为master，其他为slave。</p><h2 id="1-安装并使用Docker"><a href="#1-安装并使用Docker" class="headerlink" title="1. 安装并使用Docker"></a>1. 安装并使用Docker</h2><p>略（Todo 安装并使用Docker）</p><h2 id="2-制作镜像"><a href="#2-制作镜像" class="headerlink" title="2. 制作镜像"></a>2. 制作镜像</h2><p>从基础镜像（Ubuntu）新建一个容器</p><pre><code class="hljs applescript">sudo docker <span class="hljs-built_in">run</span> -<span class="hljs-keyword">it</span> ubuntu</code></pre><h3 id="2-1-安装Scala与Java"><a href="#2-1-安装Scala与Java" class="headerlink" title="2.1 安装Scala与Java"></a>2.1 安装Scala与Java</h3><pre><code class="hljs mipsasm">apt updateapt <span class="hljs-keyword">install </span>-y <span class="hljs-keyword">scala </span>openjdk<span class="hljs-number">-8</span>-<span class="hljs-keyword">jdk</span></code></pre><h3 id="2-2-安装ssh服务及客户端"><a href="#2-2-安装ssh服务及客户端" class="headerlink" title="2.2 安装ssh服务及客户端"></a>2.2 安装ssh服务及客户端</h3><pre><code class="hljs sql">apt <span class="hljs-keyword">install</span> -y openssh-<span class="hljs-keyword">server</span> openssh-<span class="hljs-keyword">client</span></code></pre><p>ssh安装完成后生成密钥对</p><pre><code class="hljs nginx"><span class="hljs-attribute">cd</span> ~ssh-keygen -t rsa</code></pre><p>一直回车直到密钥对创建完成，将公钥放入<code>~/.ssh/authorized_keys</code>中就可以容器间互相免密登录了</p><pre><code class="hljs jboss-cli">cat <span class="hljs-string">.ssh/id_rsa.pub</span> &gt;&gt; <span class="hljs-string">.ssh/authorized_keys</span></code></pre><p>修改 .bashrc 文件，启动 shell 的时候，自动启动 SSH 服务</p><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> <span class="hljs-string">'service ssh start'</span> &gt;&gt; ~/.bashrc</code></pre><h3 id="2-3-安装Hadoop"><a href="#2-3-安装Hadoop" class="headerlink" title="2.3 安装Hadoop"></a>2.3 安装Hadoop</h3><p>下载 Hadoop 的安装文件</p><pre><code class="hljs awk">wget http:<span class="hljs-regexp">//mi</span>rrors.hust.edu.cn<span class="hljs-regexp">/apache/</span>hadoop<span class="hljs-regexp">/common/</span>hadoop-<span class="hljs-number">3.2</span>.<span class="hljs-number">1</span><span class="hljs-regexp">/hadoop-3.2.1.tar.gz</span></code></pre><p>解压到 /usr/local 目录下面并重命名文件夹</p><pre><code class="hljs angelscript">tar -zxvf hadoop<span class="hljs-number">-3.2</span><span class="hljs-number">.1</span>.tar.gz -C /usr/local/cd /usr/local/mv hadoop<span class="hljs-number">-3.2</span><span class="hljs-number">.0</span> hadoop</code></pre><p>安装Vim并修改 /etc/profile 文件，添加一下环境变量到文件中</p><pre><code class="hljs vim">apt install -<span class="hljs-keyword">y</span> <span class="hljs-keyword">vim</span><span class="hljs-keyword">vim</span> /etc/<span class="hljs-keyword">profile</span></code></pre><p>追加以下内容</p><pre><code class="hljs routeros"><span class="hljs-comment">#java</span><span class="hljs-builtin-name">export</span> <span class="hljs-attribute">JAVA_HOME</span>=/usr/lib/jvm/java-8-openjdk-amd64<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">JRE_HOME</span>=<span class="hljs-variable">$&#123;JAVA_HOME&#125;</span>/jre    <span class="hljs-builtin-name">export</span> <span class="hljs-attribute">CLASSPATH</span>=.:$&#123;JAVA_HOME&#125;/lib:<span class="hljs-variable">$&#123;JRE_HOME&#125;</span>/lib    <span class="hljs-builtin-name">export</span> <span class="hljs-attribute">PATH</span>=<span class="hljs-variable">$&#123;JAVA_HOME&#125;</span>/bin:$PATH<span class="hljs-comment">#hadoop</span><span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HADOOP_HOME</span>=/usr/local/hadoop<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">PATH</span>=<span class="hljs-variable">$PATH</span>:$HADOOP_HOME/bin:$HADOOP_HOME/sbin<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HADOOP_COMMON_HOME</span>=<span class="hljs-variable">$HADOOP_HOME</span> <span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HADOOP_HDFS_HOME</span>=<span class="hljs-variable">$HADOOP_HOME</span> <span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HADOOP_MAPRED_HOME</span>=<span class="hljs-variable">$HADOOP_HOME</span><span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HADOOP_YARN_HOME</span>=<span class="hljs-variable">$HADOOP_HOME</span> <span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HADOOP_INSTALL</span>=<span class="hljs-variable">$HADOOP_HOME</span> <span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HADOOP_COMMON_LIB_NATIVE_DIR</span>=<span class="hljs-variable">$HADOOP_HOME</span>/lib/native <span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HADOOP_CONF_DIR</span>=<span class="hljs-variable">$HADOOP_HOME</span> <span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HADOOP_LIBEXEC_DIR</span>=<span class="hljs-variable">$HADOOP_HOME</span>/libexec <span class="hljs-builtin-name">export</span> <span class="hljs-attribute">JAVA_LIBRARY_PATH</span>=<span class="hljs-variable">$HADOOP_HOME</span>/lib/native:$JAVA_LIBRARY_PATH<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HADOOP_CONF_DIR</span>=<span class="hljs-variable">$HADOOP_PREFIX</span>/etc/hadoop<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HDFS_DATANODE_USER</span>=root<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HDFS_DATANODE_SECURE_USER</span>=root<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HDFS_SECONDARYNAMENODE_USER</span>=root<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HDFS_NAMENODE_USER</span>=root<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">YARN_RESOURCEMANAGER_USER</span>=root<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">YARN_NODEMANAGER_USER</span>=root</code></pre><p>使环境变量生效</p><pre><code class="hljs gradle"><span class="hljs-keyword">source</span> <span class="hljs-regexp">/etc/</span>profile</code></pre><p>追加/usr/local/hadoop/etc/hadoop/hadoop-env.sh文件，添加环境变量</p><pre><code class="hljs routeros"><span class="hljs-builtin-name">export</span> <span class="hljs-attribute">JAVA_HOME</span>=/usr/lib/jvm/java-8-openjdk-amd64<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HDFS_NAMENODE_USER</span>=root<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HDFS_DATANODE_USER</span>=root<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HDFS_SECONDARYNAMENODE_USER</span>=root<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">YARN_RESOURCEMANAGER_USER</span>=root<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">YARN_NODEMANAGER_USER</span>=root</code></pre><p>修改 /usr/local/hadoop/etc/hadoop/core-site.xml，添加配置</p><pre><code class="hljs dts"><span class="hljs-params">&lt;configuration&gt;</span>    <span class="hljs-params">&lt;property&gt;</span>        <span class="hljs-params">&lt;name&gt;</span>fs.default.name<span class="hljs-params">&lt;/name&gt;</span>        <span class="hljs-params">&lt;value&gt;</span>hdfs:<span class="hljs-comment">//m1:9000&lt;/value&gt;</span>    <span class="hljs-params">&lt;/property&gt;</span>    <span class="hljs-params">&lt;property&gt;</span>        <span class="hljs-params">&lt;name&gt;</span>hadoop.tmp.dir<span class="hljs-params">&lt;/name&gt;</span>        <span class="hljs-params">&lt;value&gt;</span><span class="hljs-meta-keyword">/home/</span>hadoop3<span class="hljs-meta-keyword">/hadoop/</span>tmp<span class="hljs-params">&lt;/value&gt;</span>    <span class="hljs-params">&lt;/property&gt;</span><span class="hljs-params">&lt;/configuration&gt;</span></code></pre><p>修改 /usr/local/hadoop/etc/hadoop/hdfs-site.xml<br>dfs.replication 表示备份的数量，本例设置为2份</p><pre><code class="hljs dts"><span class="hljs-params">&lt;configuration&gt;</span>    <span class="hljs-params">&lt;property&gt;</span>        <span class="hljs-params">&lt;name&gt;</span>dfs.replication<span class="hljs-params">&lt;/name&gt;</span>        <span class="hljs-params">&lt;value&gt;</span><span class="hljs-number">2</span><span class="hljs-params">&lt;/value&gt;</span>    <span class="hljs-params">&lt;/property&gt;</span>    <span class="hljs-params">&lt;property&gt;</span>        <span class="hljs-params">&lt;name&gt;</span>dfs.namenode.name.dir<span class="hljs-params">&lt;/name&gt;</span>        <span class="hljs-params">&lt;value&gt;</span><span class="hljs-meta-keyword">/home/</span>hadoop3<span class="hljs-meta-keyword">/hadoop/</span>hdfs/name<span class="hljs-params">&lt;/value&gt;</span>    <span class="hljs-params">&lt;/property&gt;</span>    <span class="hljs-params">&lt;property&gt;</span>        <span class="hljs-params">&lt;name&gt;</span>dfs.namenode.data.dir<span class="hljs-params">&lt;/name&gt;</span>        <span class="hljs-params">&lt;value&gt;</span><span class="hljs-meta-keyword">/home/</span>hadoop3<span class="hljs-meta-keyword">/hadoop/</span>hdfs/data<span class="hljs-params">&lt;/value&gt;</span>    <span class="hljs-params">&lt;/property&gt;</span><span class="hljs-params">&lt;/configuration&gt;</span></code></pre><p>修改 /usr/local/hadoop/etc/hadoop/mapred-site.xml</p><pre><code class="hljs crystal">&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;mapreduce.framework.name&lt;<span class="hljs-regexp">/name&gt;</span><span class="hljs-regexp">        &lt;value&gt;yarn&lt;/value</span>&gt;    &lt;<span class="hljs-regexp">/property&gt;</span><span class="hljs-regexp">    &lt;property&gt;</span><span class="hljs-regexp">        &lt;name&gt;mapreduce.application.classpath&lt;/name</span>&gt;        &lt;value&gt;            /usr/local/hadoop/etc/hadoop,            /usr/local/hadoop/share/hadoop/common/*,            /usr/local/hadoop/share/hadoop/common/<span class="hljs-class"><span class="hljs-keyword">lib</span>/*,</span>            /usr/local/hadoop/share/hadoop/hdfs/*,            /usr/local/hadoop/share/hadoop/hdfs/<span class="hljs-class"><span class="hljs-keyword">lib</span>/*,</span>            /usr/local/hadoop/share/hadoop/mapreduce/*,            /usr/local/hadoop/share/hadoop/mapreduce/<span class="hljs-class"><span class="hljs-keyword">lib</span>/*,</span>            /usr/local/hadoop/share/hadoop/yarn/*,            /usr/local/hadoop/share/hadoop/yarn/<span class="hljs-class"><span class="hljs-keyword">lib</span>/*</span>        &lt;<span class="hljs-regexp">/value&gt;</span><span class="hljs-regexp">    &lt;/property</span>&gt;&lt;<span class="hljs-regexp">/configuration&gt;</span></code></pre><p>修改 /usr/local/hadoop/etc/hadoop/yarn-site.xml，配置master name</p><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>m1<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>mapreduce_shuffle<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span></code></pre><p>最后修改 /usr/local/hadoop/etc/hadoop/workers 指定slave name</p><pre><code class="hljs mipsasm"><span class="hljs-built_in">s1</span><span class="hljs-built_in">s2</span><span class="hljs-built_in">s3</span><span class="hljs-built_in">s4</span></code></pre><p>此时，hadoop已经配置好了</p><h3 id="2-4-提交镜像"><a href="#2-4-提交镜像" class="headerlink" title="2.4 提交镜像"></a>2.4 提交镜像</h3><pre><code class="hljs armasm"><span class="hljs-symbol">sudo</span> docker commit -m <span class="hljs-string">"haddop"</span> <span class="hljs-keyword">b95a8ccf5568 </span>hadoop</code></pre><h2 id="3-启动Hadoop集群"><a href="#3-启动Hadoop集群" class="headerlink" title="3. 启动Hadoop集群"></a>3. 启动Hadoop集群</h2><h3 id="3-1-创建Hadoop集群容器"><a href="#3-1-创建Hadoop集群容器" class="headerlink" title="3.1 创建Hadoop集群容器"></a>3.1 创建Hadoop集群容器</h3><p>首先我们需要创建一个 Hadoop 集群的虚拟网络，现在的 Docker 网络能够提供 DNS 解析功能，我们可以使用如下命令构建。</p><pre><code class="hljs routeros">sudo docker<span class="hljs-built_in"> network </span>create <span class="hljs-attribute">--driver</span>=bridge hadoop</code></pre><p>然后启动<code>m1</code>作为Hadoop的master容器</p><pre><code class="hljs angelscript">sudo docker run -itd --network hadoop -h m1 --name <span class="hljs-string">"m1"</span> -p <span class="hljs-number">9870</span>:<span class="hljs-number">9870</span> -p <span class="hljs-number">8088</span>:<span class="hljs-number">8088</span> hadoop</code></pre><p>接下来启动4个slave容器: s1, s2, s3, s4</p><pre><code class="hljs armasm"><span class="hljs-symbol">sudo</span> docker run -<span class="hljs-keyword">itd </span>--network hadoop -h <span class="hljs-built_in">s1</span> --name <span class="hljs-string">"s1"</span> hadoop<span class="hljs-symbol">sudo</span> docker run -<span class="hljs-keyword">itd </span>--network hadoop -h <span class="hljs-built_in">s2</span> --name <span class="hljs-string">"s2"</span> hadoop<span class="hljs-symbol">sudo</span> docker run -<span class="hljs-keyword">itd </span>--network hadoop -h <span class="hljs-built_in">s3</span> --name <span class="hljs-string">"s3"</span> hadoop<span class="hljs-symbol">sudo</span> docker run -<span class="hljs-keyword">itd </span>--network hadoop -h <span class="hljs-built_in">s4</span> --name <span class="hljs-string">"s4"</span> hadoop</code></pre><h3 id="3-2-启动Hadoop集群"><a href="#3-2-启动Hadoop集群" class="headerlink" title="3.2 启动Hadoop集群"></a>3.2 启动Hadoop集群</h3><p>进入master中格式化HDFS</p><pre><code class="hljs jboss-cli">sudo docker attach m1<span class="hljs-keyword">cd</span> <span class="hljs-string">/usr/local/hadoop/bin</span><span class="hljs-string">./hadoop</span> namenode -format</code></pre><p>启动Hadoop进程</p><pre><code class="hljs jboss-cli"><span class="hljs-keyword">cd</span> <span class="hljs-string">/usr/local/hadoop/sbin/</span><span class="hljs-string">./start-all.sh</span></code></pre><p>即能看到输出表明集群启动成功，访问本机的 8088 与 9870 端口就可以看到监控信息了</p><pre><code class="hljs groovy">Starting namenodes on [m1]<span class="hljs-string">m1:</span> <span class="hljs-string">Warning:</span> Permanently added <span class="hljs-string">'m1,172.18.0.2'</span> (ECDSA) to the list of known hosts.Starting datanodes<span class="hljs-string">s1:</span> <span class="hljs-string">Warning:</span> Permanently added <span class="hljs-string">'s1,172.18.0.3'</span> (ECDSA) to the list of known hosts.<span class="hljs-string">s3:</span> <span class="hljs-string">Warning:</span> Permanently added <span class="hljs-string">'s3,172.18.0.5'</span> (ECDSA) to the list of known hosts.<span class="hljs-string">s2:</span> <span class="hljs-string">Warning:</span> Permanently added <span class="hljs-string">'s2,172.18.0.4'</span> (ECDSA) to the list of known hosts.<span class="hljs-string">s4:</span> <span class="hljs-string">Warning:</span> Permanently added <span class="hljs-string">'s4,172.18.0.6'</span> (ECDSA) to the list of known hosts.<span class="hljs-string">s4:</span> <span class="hljs-string">WARNING:</span> <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/hadoop/</span>logs does not exist. Creating.<span class="hljs-string">s2:</span> <span class="hljs-string">WARNING:</span> <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/hadoop/</span>logs does not exist. Creating.<span class="hljs-string">s3:</span> <span class="hljs-string">WARNING:</span> <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/hadoop/</span>logs does not exist. Creating.<span class="hljs-string">s1:</span> <span class="hljs-string">WARNING:</span> <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/hadoop/</span>logs does not exist. Creating.Starting secondary namenodes [m1]Starting resourcemanagerStarting nodemanagers</code></pre><p><img src="https://i.loli.net/2020/06/17/3MtX5NcE96FngaQ.jpg" srcset="/img/loading.gif" alt="localhost:8088"></p>]]></content>
    
    
    <categories>
      
      <category>框架</category>
      
      <category>Hadoop</category>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hadoop</tag>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MapReduce简介</title>
    <link href="/2020/06/17/MapReduce%E7%AE%80%E4%BB%8B/"/>
    <url>/2020/06/17/MapReduce%E7%AE%80%E4%BB%8B/</url>
    
    <content type="html"><![CDATA[<h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>MapReduce是Google提出的一个软件架构，用于大数据的并行运算。其中”Map(映射)”，”Reduce(归纳)”和主要思想均从函数式编程借鉴来的。</p><p>每个文件分片由单独的机器去处理，这就是Map的方法；将各个机器计算的结果汇总并得到最终的结果，这就是Reduce的方法。</p><h3 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h3><p>向MapReduce框架提交一个计算作业时，它会首先把计算作业拆分成若干个Map任务，然后分配到不同的节点上去执行，每一个Map任务处理输入数据中的一部分，当Map任务完成后，它会生成一些中间文件，这些中间文件将会作为Reduce任务的输入数据。Reduce任务的主要目标就是把前面若干个Map的输出汇总到一起并输出。</p><p><img src="https://cs.calvin.edu/courses/cs/374/exercises/12/lab/MapReduceWordCount.png" srcset="/img/loading.gif" alt="MapReduce工作流程"></p>]]></content>
    
    
    <categories>
      
      <category>框架</category>
      
      <category>Hadoop</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hadoop</tag>
      
      <tag>MapReduce</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HDFS框架</title>
    <link href="/2020/06/16/HDFS%E6%A1%86%E6%9E%B6/"/>
    <url>/2020/06/16/HDFS%E6%A1%86%E6%9E%B6/</url>
    
    <content type="html"><![CDATA[<h3 id="HDFS框架分析"><a href="#HDFS框架分析" class="headerlink" title="HDFS框架分析"></a>HDFS框架分析</h3><p>HDFS和大多数分布式框架结构一致，都是由master（Name-Node、Secondary NameNode）/slave（DataNode）构成。</p><p><img src="https://pic2.zhimg.com/80/v2-5ec14190630b72a07a242787a1e874cd_1440w.jpg" srcset="/img/loading.gif" alt="HDFS结构"></p><h3 id="master的主要功能"><a href="#master的主要功能" class="headerlink" title="master的主要功能"></a>master的主要功能</h3><ul><li>管理HDFS文件系统的namespace，负责client请求（读取、存储）的相应</li><li>管理文件存储到slave节点上（包含数据副本的分配）</li><li>按配置（blocksize）项拆分文件成多个block（默认128M）存储到多个slave节点上</li></ul><h3 id="slave的主要功能"><a href="#slave的主要功能" class="headerlink" title="slave的主要功能"></a>slave的主要功能</h3><ul><li>负责block（数据块）的创建、删除以及副本存储工作</li><li>定期向master发送心跳信息汇报节点状态</li></ul><h3 id="HDFS的master容错机制"><a href="#HDFS的master容错机制" class="headerlink" title="HDFS的master容错机制"></a>HDFS的master容错机制</h3><p>Hadoop可以配置高可用集群，集群中有两个master节点，一台Name-Node主节点，另一台Secondary NameNode备用节点，两者数据时刻保持一致。当主节点不可用时，备用节点马上自动切换，用户感知不到，避免了master的单点问题。</p><h3 id="HDFS的数据块副本机制"><a href="#HDFS的数据块副本机制" class="headerlink" title="HDFS的数据块副本机制"></a>HDFS的数据块副本机制</h3><p>超过一定大小（blocksize）的文件会被拆分成多个block存储在不同的slave里。HDFS为了容错会把一个block存储在多个slave当中，而一个block存到几个slave当中是由副本因子（replication factor）配置的。</p><p><img src="https://pic3.zhimg.com/80/v2-00887ec98b314356fd05c0b60df6057a_1440w.jpg" srcset="/img/loading.gif" alt="HDFS副本机制"></p><h3 id="HDFS读文件流程"><a href="#HDFS读文件流程" class="headerlink" title="HDFS读文件流程"></a>HDFS读文件流程</h3><ul><li>由client向master查询元数据，找到文件的block所在的slave</li><li>挑选一台slave服务器建立连接</li><li>client按packet为单位接收并缓存，最终合成所需文件</li></ul><h3 id="HDFS写文件流程"><a href="#HDFS写文件流程" class="headerlink" title="HDFS写文件流程"></a>HDFS写文件流程</h3><ul><li>由client向master发送上传文件请求</li><li>client按master要求拆分文件</li><li>client向master所给的slave上传第一个block</li><li>slave接收完block校验并发送至其他slave做副本备份</li><li>client再次向master请求下一个block上传的slave</li></ul>]]></content>
    
    
    <categories>
      
      <category>框架</category>
      
      <category>Hadoop</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hadoop</tag>
      
      <tag>HDFS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>初识Hadoop</title>
    <link href="/2020/06/16/%E5%88%9D%E8%AF%86Hadoop/"/>
    <url>/2020/06/16/%E5%88%9D%E8%AF%86Hadoop/</url>
    
    <content type="html"><![CDATA[<h2 id="Hadoop是啥"><a href="#Hadoop是啥" class="headerlink" title="Hadoop是啥"></a>Hadoop是啥</h2><p>Hadoop是一个开源大数据框架，是一个分布式计算的解决方案。</p><p>Hadoop解决了大数据存储（HDFS文件系统）和分布式计算（MapReduce）两大难题。</p><p><img src="http://www.sunlab.org/teaching/cse8803/fall2016/lab/image/post/mapreduce-flow.jpg" srcset="/img/loading.gif" alt="Hadoop结构"></p><h2 id="Hadoop特点"><a href="#Hadoop特点" class="headerlink" title="Hadoop特点"></a>Hadoop特点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul><li>HDFS<ul><li>支持超大文件至PB级别</li><li>分布式可快速扩容呈线性扩展</li><li>有冗余备份</li><li>具有检测和快速应对故障的能力</li></ul></li><li>MapReduce<ul><li>开发简单</li><li>可扩展性强</li><li>对节点故障容错性强</li></ul></li></ul><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul><li>HDFS<ul><li>不适合大量小文件存储</li><li>文件修改效率低</li><li>速度慢</li></ul></li><li>MapReduce<ul><li>执行时间长</li><li>Map和Reduce函数过于底层</li><li>不能实现一些算法</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>框架</category>
      
      <category>Hadoop</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hadoop</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
